{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {Gaussian, Uniform} to 8-Gaussian, guidance with Learned, MC, CEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "from guided_flow.backbone.mlp import MLP\n",
    "from guided_flow.backbone.wrapper import ExpEnergyMLPWrapper, GuidedMLPWrapper, MLPWrapper\n",
    "from guided_flow.config.sampling import GuideFnConfig\n",
    "from guided_flow.distributions.base import BaseDistribution, get_distribution\n",
    "from guided_flow.distributions.gaussian import GaussianDistribution\n",
    "from guided_flow.flow.optimal_transport import OTPlanSampler\n",
    "from guided_flow.guidance.gradient_guidance import wrap_grad_fn\n",
    "from guided_flow.utils.misc import deterministic\n",
    "from guided_flow.utils.metrics import compute_w2 as w2\n",
    "import torch\n",
    "from torchdyn.core import NeuralODE\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, Independent\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from guided_flow.config.sampling import ODEConfig\n",
    "\n",
    "\n",
    "from guided_flow.utils.kl_divergence import compute_kl_divergence\n",
    "MLP_WIDTH = 256\n",
    "TRAINING_B = 256 # OT CFM training batch size\n",
    "\n",
    "\n",
    "def sample_x1_frompeJ(x1_sampler, x1_dist, device, B):\n",
    "    x1 = None\n",
    "    while x1 is None or x1.shape[0] < B:\n",
    "        x1_ = x1_sampler(B).to(device)\n",
    "        weights = torch.exp(-x1_dist.get_J(x1_))\n",
    "        acc_prob = weights / weights.max()\n",
    "        random_numbers = torch.rand(B, device=device)\n",
    "        x1_ = x1_[random_numbers < acc_prob]\n",
    "        if x1 is None:\n",
    "            x1 = x1_\n",
    "        else:\n",
    "            x1 = torch.cat([x1, x1_], 0)\n",
    "    x1 = x1[:B]\n",
    "    return x1\n",
    "\n",
    "\n",
    "def compute_w2(trajs, cfgs: List[GuideFnConfig], J_weighted=True):\n",
    "    w2s = []\n",
    "    for traj, cfg in zip(trajs, cfgs):\n",
    "        x0_dist = get_distribution(cfg.dist_pair[0])\n",
    "        x1_dist = get_distribution(cfg.dist_pair[1])\n",
    "        \n",
    "        if J_weighted:\n",
    "            x1 = sample_x1_frompeJ(x1_dist.sample, x1_dist, cfg.ode_cfg.device, cfg.ode_cfg.batch_size)\n",
    "        else:\n",
    "            x1 = x1_dist.sample(cfg.ode_cfg.batch_size).to(cfg.ode_cfg.device)\n",
    "        w2s.append(w2(traj[-1], x1))\n",
    "    return w2s\n",
    "\n",
    "\n",
    "def get_mc_guide_fn(x0_dist: BaseDistribution, x1_dist: BaseDistribution, mc_cfg: GuideFnConfig, cfm: str):\n",
    "\n",
    "    def log_cfm_p_t1(x1, xt, t):\n",
    "        # xt = t x1 + (1 - t) x0 -> x0 = xt / (1 - t) - t / (1 - t) x1\n",
    "        x0 = xt / (1 - t + mc_cfg.ep) - (t + mc_cfg.ep) / (1 - t + mc_cfg.ep) * x1 # (B, 2)\n",
    "        p1t = x0_dist.prob(x0).clamp(1e-8) / (1 - t[0] + mc_cfg.ep) ** 2 # (B,)\n",
    "        log_p1t = p1t.log()\n",
    "        # print(log_p1t.mean())\n",
    "        return log_p1t\n",
    "        \n",
    "    def ot_cfm_log_p_tz(x0, x1, xt, t, std=None):\n",
    "        mean = t * x1 + (1 - t) * x0 # (B, 2)\n",
    "         # g.t. std: 0. Too small: requires large mc_batch_size; Too large: inaccurate\n",
    "        base_dist = Normal(loc=mean, scale=std)\n",
    "        distribution = Independent(base_dist, 1)\n",
    "        log_p1t = distribution.log_prob(xt) # (B,)\n",
    "        return log_p1t\n",
    "        \n",
    "    def guide_fn(t, x, dx_dt, model, x0=None, x1=None, Jx1=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: Tensor, shape (b, 1)\n",
    "            x: Tensor, shape (b, dim)\n",
    "            dx_dt: Tensor, shape (b, dim)\n",
    "            model: MLP\n",
    "        \"\"\"\n",
    "        # estimate E (e^{-J} / Z - 1) * u\n",
    "        b = x.shape[0]\n",
    "        B = mc_cfg.mc_batch_size\n",
    "        x_ = x.repeat(B, 1) # (MC_B * b, 2)\n",
    "        t_ = t.repeat(B * b, 1) # (MC_B * b)\n",
    "        if cfm == 'cfm':\n",
    "            log_p_t1_x = log_cfm_p_t1(x1, x_, t_) # (MC_B * b) # TODO\n",
    "            log_p_t_x = log_p_t1_x.reshape(B, b, 1).logsumexp(0) - torch.log(torch.tensor(B, device=x.device)) # (MC_B, B, 1) -> (B, 1)\n",
    "            log_p_t1_x_times_J_ = (log_p_t1_x + torch.log(Jx1)).reshape(B, b, 1) # (MC_B * b) -> (MC_B, b, 1)            \n",
    "            logZ = torch.logsumexp(log_p_t1_x_times_J_, 0) - torch.log(torch.tensor(B, device=x.device)) - log_p_t_x # (b, 1)\n",
    "\n",
    "            Z = torch.exp(logZ)\n",
    "            u = (x1 - x_) / (1 - t_ + mc_cfg.ep) # (MC_B * b, dim)\n",
    "\n",
    "            g = (log_p_t1_x.reshape(B, b, 1) - log_p_t_x.unsqueeze(0)).exp() * (Jx1.reshape(B, b, 1) / (Z + 1e-8).unsqueeze(0) - 1) * u.reshape(B, b, 2) # (MC_B, b, dim)\n",
    "\n",
    "            return g.mean(0)\n",
    "        \n",
    "        elif cfm == 'ot_cfm':\n",
    "            log_p_tz_x = ot_cfm_log_p_tz(x0, x1, x_, t_, std=mc_cfg.ot_std) # (MC_B * b)\n",
    "            log_p_t_x = log_p_tz_x.reshape(B, b, 1).logsumexp(0) - torch.log(torch.tensor(B, device=x.device)) # (MC_B, b, 1) -> (b, 1)\n",
    "            log_p_tz_x_times_J_ = (log_p_tz_x + torch.log(Jx1)).reshape(B, b, 1) # (MC_B * b) -> (MC_B, b, 1)\n",
    "            \n",
    "            logZ = torch.logsumexp(log_p_tz_x_times_J_, 0) - torch.log(torch.tensor(B, device=x.device)) - log_p_t_x # (b, 1)\n",
    "            \n",
    "            Z = torch.exp(logZ)\n",
    "            u = x1 - x0 # (MC_B * b, dim)\n",
    "            \n",
    "            g = (log_p_tz_x.reshape(B, b, 1) - log_p_t_x.unsqueeze(0)).exp() * (Jx1.reshape(B, b, 1) / Z - 1) * u.reshape(B, b, 2) # (MC_B, b, dim)\n",
    "            \n",
    "            return g.mean(0)\n",
    "\n",
    "    \n",
    "    if cfm == 'cfm':\n",
    "        x1 = x1_dist.sample(mc_cfg.mc_batch_size).to(mc_cfg.ode_cfg.device).unsqueeze(0).repeat(mc_cfg.ode_cfg.batch_size, 1, 1).permute(1, 0, 2).reshape(-1, 2)\n",
    "        Jx1 = torch.exp(-mc_cfg.scale * x1_dist.get_J(x1))\n",
    "        return partial(\n",
    "            guide_fn, \n",
    "            x1=x1, \n",
    "            Jx1=Jx1\n",
    "        )\n",
    "    elif cfm == 'ot_cfm':\n",
    "        x0 = x0_dist.sample(mc_cfg.mc_batch_size) # (MC_B, 2)\n",
    "        x1 = x1_dist.sample(mc_cfg.mc_batch_size) # (MC_B, 2)\n",
    "        x0_ = x0.to(mc_cfg.ode_cfg.device).unsqueeze(0).repeat(mc_cfg.ode_cfg.batch_size, 1, 1).permute(1, 0, 2).reshape(-1, 2)\n",
    "        x1_ = x1.to(mc_cfg.ode_cfg.device).unsqueeze(0).repeat(mc_cfg.ode_cfg.batch_size, 1, 1).permute(1, 0, 2).reshape(-1, 2)\n",
    "        J_ = torch.exp(-mc_cfg.scale * x1_dist.get_J(x1_)) # (MC_B * b)\n",
    "        \n",
    "        return partial(\n",
    "            guide_fn, \n",
    "            x0=x0_, \n",
    "            x1=x1_, \n",
    "            Jx1=J_\n",
    "        )\n",
    "\n",
    "def get_guide_fn(dist: BaseDistribution, cfg: GuideFnConfig):\n",
    "    def guide_fn(t, x, dx_dt, model):\n",
    "\n",
    "        if cfg.guide_type == 'g_cov_A':\n",
    "            x1_pred = x + dx_dt * (1 - t)\n",
    "            J = dist.get_J(x1_pred)\n",
    "            try:\n",
    "                with torch.enable_grad():\n",
    "                    x1_pred = x1_pred.requires_grad_(True)\n",
    "                    J = dist.get_J(x1_pred)\n",
    "                    grad = -torch.autograd.grad(J.sum(), x1_pred, create_graph=True)[0]\n",
    "                    return grad\n",
    "            except Exception as e:\n",
    "                return torch.zeros_like(x)\n",
    "        \n",
    "        elif cfg.guide_type == 'g_cov_G':\n",
    "            with torch.enable_grad():\n",
    "                x = x.requires_grad_(True)\n",
    "                x1_pred = x + model(torch.cat([x, t.repeat(x.shape[0])[:, None]], 1)) * (1 - t)\n",
    "                J = dist.get_J(x1_pred)\n",
    "                try:\n",
    "                    grad = -torch.autograd.grad(J.sum(), x, create_graph=True)[0]\n",
    "                    return grad\n",
    "                except Exception as e:\n",
    "                    return torch.zeros_like(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown guide function: {cfg.guide_type}\")\n",
    "    # make scale and schedule\n",
    "    return wrap_grad_fn(cfg.guide_scale, cfg.guide_schedule, guide_fn)\n",
    "\n",
    "\n",
    "def get_sim_mc_guide_fn(x1_dist: BaseDistribution, cfg: GuideFnConfig):\n",
    "    def guide_fn(t, x, dx_dt, model):\n",
    "        \"\"\"\n",
    "        Implements guidance following Eq. 12\n",
    "        Args:\n",
    "            t: flow time. float\n",
    "            x: current sample x_t. Tensor, shape (b, dim)\n",
    "            dx_dt: current predicted VF. Tensor, shape (b, dim)\n",
    "            model: flow model. MLP\n",
    "        \"\"\"\n",
    "        x1_pred = x + dx_dt * (1 - t) # (B, 2)\n",
    "        std = cfg.sim_mc_std\n",
    "        \n",
    "        x1 = torch.randn_like(x1_pred.unsqueeze(0).repeat(cfg.sim_mc_n, 1, 1)) * std + x1_pred # (cfg.sim_mc_n, B, 2)\n",
    "        Jx1_ = torch.exp(-cfg.scale * x1_dist.get_J(x1.reshape(-1, 2))).reshape(cfg.sim_mc_n, -1) # (cfg.sim_mc_n, B)\n",
    "        v = (x1 - x) / (1 - t + cfg.ep)  # Conditional VF v_{t|z} in Eq. 12 (cfg.sim_mc_n, B, 2)\n",
    "        Z = Jx1_.mean(0) + 1e-8  # Z in Eq. 12 (B,)\n",
    "        g = (Jx1_ / Z - 1).unsqueeze(2) * v  # g in Eq. 12 (cfg.sim_mc_n, B, 2)\n",
    "        return g.mean(0)\n",
    "    return wrap_grad_fn(cfg.guide_scale, cfg.guide_schedule, guide_fn)\n",
    "\n",
    "def evaluate(x0_sampler, x1_sampler, model, guide_fn, cfg: ODEConfig):\n",
    "    node = NeuralODE(\n",
    "        GuidedMLPWrapper(\n",
    "            model, \n",
    "            guide_fn=guide_fn,\n",
    "            scheduler=lambda t: 1\n",
    "        ), \n",
    "        solver=\"euler\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        traj = node.trajectory(\n",
    "            x0_sampler(cfg.batch_size).to(cfg.device), \n",
    "            t_span=torch.linspace(0, cfg.t_end, cfg.num_steps)\n",
    "        )\n",
    "    \n",
    "    return traj\n",
    "\n",
    "\n",
    "def sample_and_compute_w2(guide_cfgs: List[GuideFnConfig]):\n",
    "    print(\"Monte Carlo batch size:\", guide_cfgs[0].mc_batch_size)\n",
    "\n",
    "    trajs = []\n",
    "\n",
    "    for cfg in guide_cfgs:\n",
    "\n",
    "        # Initialize samplers, model and guidance model\n",
    "        x0_dist = get_distribution(cfg.dist_pair[0])\n",
    "        x1_dist = get_distribution(cfg.dist_pair[1])\n",
    "\n",
    "        x0_sampler = x0_dist.sample\n",
    "        x1_sampler = x1_dist.sample\n",
    "\n",
    "        model = MLP(dim=2, w=MLP_WIDTH, time_varying=True).to(cfg.ode_cfg.device)\n",
    "        model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "\n",
    "        if cfg.guide_type == 'mc':\n",
    "            # sample using flow model\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, get_mc_guide_fn(x0_dist, x1_dist, cfg, cfg.cfm), cfg.ode_cfg)\n",
    "            \n",
    "        elif cfg.guide_type == 'learned':\n",
    "            model_G = MLP(dim=2, out_dim=2, w=MLP_WIDTH, time_varying=True).to(cfg.ode_cfg.device)\n",
    "            model_G.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/guidance_matching_{cfg.gm_type}_scale_{cfg.scale}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, MLPWrapper(model_G, scheduler=lambda t: 1., clamp=0), cfg.ode_cfg)\n",
    "        \n",
    "        elif cfg.guide_type == 'ceg':\n",
    "            model_Z = MLP(dim=2, out_dim=1, w=MLP_WIDTH, time_varying=True, exp_final=False).to(cfg.ode_cfg.device)\n",
    "            model_Z.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/ceg_scale_{cfg.scale}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "            \n",
    "            # 2D xy plane. make uniform grid\n",
    "            XX = torch.linspace(0, 1, 100)\n",
    "            YY = torch.linspace(0, 1, 100)\n",
    "            XX, YY = torch.meshgrid(XX, YY, indexing='ij')\n",
    "            xy = torch.stack([XX.flatten(), YY.flatten()], 1)\n",
    "            t = torch.zeros(10000, 1) + 0.9\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(model_Z(torch.cat([xy, t], 1).to(cfg.ode_cfg.device)).detach().cpu().numpy().reshape(100, 100))\n",
    "            \n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, ExpEnergyMLPWrapper(model_Z, scheduler=lambda t: 1, clamp=1), cfg.ode_cfg)\n",
    "        \n",
    "        elif cfg.guide_type in ['g_cov_A', 'g_cov_G']:\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, get_guide_fn(x1_dist, cfg), cfg.ode_cfg)\n",
    "\n",
    "        elif cfg.guide_type == 'g_sim_MC':\n",
    "            traj = evaluate(x0_sampler, x1_sampler, model, get_sim_mc_guide_fn(x1_dist, cfg), cfg.ode_cfg)\n",
    "        trajs.append(traj)\n",
    "\n",
    "    w2 = compute_w2(trajs, guide_cfgs)\n",
    "    return trajs, w2\n",
    "\n",
    "def sample_and_compute_w2_uncond(cfg):\n",
    "    # Initialize samplers, model and guidance model\n",
    "        x0_dist = get_distribution(cfg.dist_pair[0])\n",
    "        x1_dist = get_distribution(cfg.dist_pair[1])\n",
    "\n",
    "        x0_sampler = x0_dist.sample\n",
    "        x1_sampler = x1_dist.sample\n",
    "\n",
    "        model = MLP(dim=2, w=MLP_WIDTH, time_varying=True).to(cfg.ode_cfg.device)\n",
    "        model.load_state_dict(torch.load(f'../logs/{cfg.dist_pair[0]}-{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}/{cfg.cfm}_{cfg.dist_pair[0]}_{cfg.dist_pair[1]}.pth'))\n",
    "\n",
    "        # sample using flow model\n",
    "        traj = evaluate(x0_sampler, x1_sampler, model, lambda *args, **kwargs: 0, cfg.ode_cfg)\n",
    "        w2 = compute_w2([traj], [cfg], J_weighted=False)\n",
    "        return w2\n",
    "        \n",
    "deterministic(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ls = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "guide_cfgs_mc_cfm = [\n",
    "    GuideFnConfig(dist_pair=('circle', 'moon'), cfm='cfm', ot_std=0.1, mc_batch_size=mc_b, ep=1e-2, ode_cfg=ODEConfig(t_end=0.97, num_steps=100, batch_size=1024)) for mc_b in b_ls\n",
    "]\n",
    "\n",
    "g_8g_results = []\n",
    "for seed in range(10):\n",
    "    deterministic(seed)\n",
    "    trajs_mc_cfm, w2s_mc_cfm = sample_and_compute_w2(guide_cfgs_mc_cfm)\n",
    "    g_8g_results.append(w2s_mc_cfm)\n",
    "\n",
    "print(g_8g_results)\n",
    "\n",
    "cfm = GuideFnConfig(dist_pair=('circle', 'moon'), mc_batch_size=0, ep=1e-2, ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=1000))\n",
    "\n",
    "g_8g_w2_uncond = []\n",
    "for seed in range(10):\n",
    "    deterministic(seed)\n",
    "    g_8g_w2_uncond.append(sample_and_compute_w2_uncond(cfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ls = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "guide_cfgs_mc_cfm = [\n",
    "    GuideFnConfig(dist_pair=('gaussian', 'moon'), mc_batch_size=mc_b, ep=1e-2, ode_cfg=ODEConfig(t_end=1, num_steps=100, batch_size=1024)) for mc_b in b_ls\n",
    "]\n",
    "\n",
    "u_s_results = []\n",
    "for seed in range(10):\n",
    "    deterministic(seed)\n",
    "    trajs_mc_cfm, w2s_mc_cfm = sample_and_compute_w2(guide_cfgs_mc_cfm)\n",
    "    u_s_results.append(w2s_mc_cfm)\n",
    "\n",
    "print(u_s_results)\n",
    "\n",
    "cfm = GuideFnConfig(dist_pair=('gaussian', 'moon'), mc_batch_size=0, ep=1e-2, ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=1000))\n",
    "\n",
    "u_s_w2_uncond = []\n",
    "for seed in range(10):\n",
    "    deterministic(seed)\n",
    "    u_s_w2_uncond.append(sample_and_compute_w2_uncond(cfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ls = [5, 10, 20, 50, 100, 200, 500, 1000, 2000]\n",
    "guide_cfgs_mc_cfm = [\n",
    "    GuideFnConfig(dist_pair=('8gaussian', 'moon'), mc_batch_size=mc_b, ep=1e-2, ode_cfg=ODEConfig(t_end=1, num_steps=100, batch_size=1024)) for mc_b in b_ls\n",
    "]\n",
    "\n",
    "m_cc_results = []\n",
    "for seed in range(10):\n",
    "    deterministic(seed)\n",
    "    trajs_mc_cfm, w2s_mc_cfm = sample_and_compute_w2(guide_cfgs_mc_cfm)\n",
    "    m_cc_results.append(w2s_mc_cfm)\n",
    "\n",
    "print(m_cc_results)\n",
    "\n",
    "cfm = GuideFnConfig(dist_pair=('8gaussian', 'moon'), mc_batch_size=0, ep=1e-2, ode_cfg=ODEConfig(t_end=1.0, num_steps=100, batch_size=1000))\n",
    "\n",
    "m_cc_w2_uncond = []\n",
    "for seed in range(10):\n",
    "    deterministic(seed)\n",
    "    m_cc_w2_uncond.append(sample_and_compute_w2_uncond(cfm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_8g_w2_uncond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and std\n",
    "\n",
    "def ls_to_plot(results, lower_bound):\n",
    "    results = np.array(results)\n",
    "    mean = results.mean(0)\n",
    "    std = results.std(0)\n",
    "    low_mean = np.array(lower_bound).mean(0)\n",
    "    return mean, std, low_mean\n",
    "\n",
    "def plot(ax, color, label, mean, std, low_mean):\n",
    "    ax.plot(b_ls, mean, marker='x', color=color, label=label)\n",
    "    ax.fill_between(b_ls, mean - std, mean + std, color=color, alpha=0.5, linewidth=1, )\n",
    "    ax.plot(b_ls, np.ones_like(b_ls) * low_mean, color=color, ls='--', linewidth=2, )\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(4, 3)\n",
    "plot(ax, 'C0', 'Circle $\\\\rightarrow$ Moons', *ls_to_plot(g_8g_results, g_8g_w2_uncond))\n",
    "plot(ax, 'C1', 'Gaussian $\\\\rightarrow$ Moons', *ls_to_plot(u_s_results, u_s_w2_uncond))\n",
    "plot(ax, 'C3', '8 Gaussians $\\\\rightarrow$ Moons', *ls_to_plot(m_cc_results, m_cc_w2_uncond))\n",
    "# ax.set_yscale('log')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xscale('log')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Number of Samples')\n",
    "ax.set_ylabel('$\\\\mathcal{W}_2$ Distance')\n",
    "\n",
    "fig.savefig('mc_n.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
